# ì—¬ëŸ¬ Ollama ì¸ìŠ¤í„´ìŠ¤ êµ¬ì„± ê°€ì´ë“œ

GPU ì„œë²„ì—ì„œ ì—¬ëŸ¬ ê°œì˜ LLM API ì„œë²„ë¥¼ ë™ì‹œì— ì‹¤í–‰í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.

## ğŸ“‹ ëª©ì°¨

- [ê°œìš”](#ê°œìš”)
- [ë‹¨ì¼ GPUì—ì„œ ì—¬ëŸ¬ ëª¨ë¸](#ë‹¨ì¼-gpuì—ì„œ-ì—¬ëŸ¬-ëª¨ë¸)
- [ì—¬ëŸ¬ Ollama ì¸ìŠ¤í„´ìŠ¤](#ì—¬ëŸ¬-ollama-ì¸ìŠ¤í„´ìŠ¤)
- [í¬íŠ¸ êµ¬ì„±](#í¬íŠ¸-êµ¬ì„±)
- [VM ì„œë²„ ì—°ê²°](#vm-ì„œë²„-ì—°ê²°)

## ê°œìš”

### ì‚¬ìš© ì‚¬ë¡€

1. **ì—¬ëŸ¬ ëª¨ë¸ ì œê³µ**
   - ì¸ìŠ¤í„´ìŠ¤ 1 (í¬íŠ¸ 8001): exaone3.5:latest
   - ì¸ìŠ¤í„´ìŠ¤ 2 (í¬íŠ¸ 8002): llama3.2:latest
   - ì¸ìŠ¤í„´ìŠ¤ 3 (í¬íŠ¸ 8003): gemma2:9b

2. **ëª¨ë¸ ë²„ì „ ê´€ë¦¬**
   - ì¸ìŠ¤í„´ìŠ¤ 1 (í¬íŠ¸ 8001): í”„ë¡œë•ì…˜ ëª¨ë¸
   - ì¸ìŠ¤í„´ìŠ¤ 2 (í¬íŠ¸ 8002): í…ŒìŠ¤íŠ¸ ëª¨ë¸

3. **ë¶€í•˜ ë¶„ì‚°**
   - ì¸ìŠ¤í„´ìŠ¤ 1-3 (í¬íŠ¸ 8001-8003): ê°™ì€ ëª¨ë¸, ë‹¤ë¥¸ ì¸ìŠ¤í„´ìŠ¤

## ë°©ë²• 1: ë‹¨ì¼ Ollama + ì—¬ëŸ¬ API ì„œë²„

### êµ¬ì¡°

```
GPU ì„œë²„
â”œâ”€â”€ Ollama (í¬íŠ¸ 11434) - ì—¬ëŸ¬ ëª¨ë¸ ë¡œë“œ
â”œâ”€â”€ LLM API Server 1 (í¬íŠ¸ 8001) - exaone3.5
â”œâ”€â”€ LLM API Server 2 (í¬íŠ¸ 8002) - llama3.2
â””â”€â”€ LLM API Server 3 (í¬íŠ¸ 8003) - gemma2
```

### 1. Ollamaì— ì—¬ëŸ¬ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ

```bash
# Ollama ì„œë¹„ìŠ¤ ì‹œì‘
ollama serve &

# ì—¬ëŸ¬ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
ollama pull exaone3.5:latest
ollama pull llama3.2:latest
ollama pull gemma2:9b

# í™•ì¸
ollama list
```

### 2. GPU ì„œë²„ ì¸ìŠ¤í„´ìŠ¤ ë³µì‚¬

```bash
cd ~
mkdir -p qrchatbot-gpu

# ì¸ìŠ¤í„´ìŠ¤ 1 (exaone3.5)
cp -r gpu-server qrchatbot-gpu/instance1
cd qrchatbot-gpu/instance1

cat > .env << EOF
OLLAMA_BASE_URL=http://127.0.0.1:11434
OLLAMA_MODEL=exaone3.5:latest
HOST=0.0.0.0
PORT=8001
EOF

# ì¸ìŠ¤í„´ìŠ¤ 2 (llama3.2)
cd ~
cp -r gpu-server qrchatbot-gpu/instance2
cd qrchatbot-gpu/instance2

cat > .env << EOF
OLLAMA_BASE_URL=http://127.0.0.1:11434
OLLAMA_MODEL=llama3.2:latest
HOST=0.0.0.0
PORT=8002
EOF

# ì¸ìŠ¤í„´ìŠ¤ 3 (gemma2)
cd ~
cp -r gpu-server qrchatbot-gpu/instance3
cd qrchatbot-gpu/instance3

cat > .env << EOF
OLLAMA_BASE_URL=http://127.0.0.1:11434
OLLAMA_MODEL=gemma2:9b
HOST=0.0.0.0
PORT=8003
EOF
```

### 3. ê° ì¸ìŠ¤í„´ìŠ¤ ì‹¤í–‰

```bash
# í„°ë¯¸ë„ 1
cd ~/qrchatbot-gpu/instance1
python run.py

# í„°ë¯¸ë„ 2
cd ~/qrchatbot-gpu/instance2
python run.py

# í„°ë¯¸ë„ 3
cd ~/qrchatbot-gpu/instance3
python run.py
```

ë˜ëŠ” ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰:

```bash
cd ~/qrchatbot-gpu/instance1 && nohup python run.py > gpu-server-8001.log 2>&1 &
cd ~/qrchatbot-gpu/instance2 && nohup python run.py > gpu-server-8002.log 2>&1 &
cd ~/qrchatbot-gpu/instance3 && nohup python run.py > gpu-server-8003.log 2>&1 &
```

### 4. Systemd ì„œë¹„ìŠ¤ë¡œ ê´€ë¦¬ (ê¶Œì¥)

**ì¸ìŠ¤í„´ìŠ¤ 1:**
```bash
sudo nano /etc/systemd/system/qrchatbot-gpu-8001.service
```

```ini
[Unit]
Description=QRChatBot GPU LLM Server - Instance 1 (exaone3.5)
After=network.target

[Service]
Type=simple
User=your-username
WorkingDirectory=/home/your-username/qrchatbot-gpu/instance1
Environment="PATH=/home/your-username/qrchatbot-gpu/instance1/venv/bin"
ExecStart=/home/your-username/qrchatbot-gpu/instance1/venv/bin/python run.py
Restart=always
RestartSec=5
StandardOutput=journal
StandardError=journal

[Install]
WantedBy=multi-user.target
```

**ì¸ìŠ¤í„´ìŠ¤ 2:**
```bash
sudo nano /etc/systemd/system/qrchatbot-gpu-8002.service
```

```ini
[Unit]
Description=QRChatBot GPU LLM Server - Instance 2 (llama3.2)
After=network.target

[Service]
Type=simple
User=your-username
WorkingDirectory=/home/your-username/qrchatbot-gpu/instance2
Environment="PATH=/home/your-username/qrchatbot-gpu/instance2/venv/bin"
ExecStart=/home/your-username/qrchatbot-gpu/instance2/venv/bin/python run.py
Restart=always
RestartSec=5
StandardOutput=journal
StandardError=journal

[Install]
WantedBy=multi-user.target
```

**ì¸ìŠ¤í„´ìŠ¤ 3:**
```bash
sudo nano /etc/systemd/system/qrchatbot-gpu-8003.service
```

```ini
[Unit]
Description=QRChatBot GPU LLM Server - Instance 3 (gemma2)
After=network.target

[Service]
Type=simple
User=your-username
WorkingDirectory=/home/your-username/qrchatbot-gpu/instance3
Environment="PATH=/home/your-username/qrchatbot-gpu/instance3/venv/bin"
ExecStart=/home/your-username/qrchatbot-gpu/instance3/venv/bin/python run.py
Restart=always
RestartSec=5
StandardOutput=journal
StandardError=journal

[Install]
WantedBy=multi-user.target
```

**ì„œë¹„ìŠ¤ ì‹œì‘:**
```bash
sudo systemctl daemon-reload
sudo systemctl enable qrchatbot-gpu-8001 qrchatbot-gpu-8002 qrchatbot-gpu-8003
sudo systemctl start qrchatbot-gpu-8001 qrchatbot-gpu-8002 qrchatbot-gpu-8003

# ìƒíƒœ í™•ì¸
sudo systemctl status qrchatbot-gpu-8001
sudo systemctl status qrchatbot-gpu-8002
sudo systemctl status qrchatbot-gpu-8003
```

## ë°©ë²• 2: ì—¬ëŸ¬ Ollama ì¸ìŠ¤í„´ìŠ¤

ë” ê²©ë¦¬ëœ í™˜ê²½ì´ í•„ìš”í•œ ê²½ìš° ê°ê° ë‹¤ë¥¸ Ollama ì¸ìŠ¤í„´ìŠ¤ ì‚¬ìš©:

### êµ¬ì¡°

```
GPU ì„œë²„
â”œâ”€â”€ Ollama 1 (í¬íŠ¸ 11434) + LLM API 1 (í¬íŠ¸ 8001)
â”œâ”€â”€ Ollama 2 (í¬íŠ¸ 11435) + LLM API 2 (í¬íŠ¸ 8002)
â””â”€â”€ Ollama 3 (í¬íŠ¸ 11436) + LLM API 3 (í¬íŠ¸ 8003)
```

### ì„¤ì •

```bash
# Ollama 1 (ê¸°ë³¸)
OLLAMA_HOST=127.0.0.1:11434 ollama serve &

# Ollama 2
OLLAMA_HOST=127.0.0.1:11435 ollama serve &

# Ollama 3
OLLAMA_HOST=127.0.0.1:11436 ollama serve &

# ê° Ollamaì— ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
OLLAMA_HOST=127.0.0.1:11434 ollama pull exaone3.5:latest
OLLAMA_HOST=127.0.0.1:11435 ollama pull llama3.2:latest
OLLAMA_HOST=127.0.0.1:11436 ollama pull gemma2:9b
```

ê° LLM API ì„œë²„ì˜ `.env`:

```bash
# instance1/.env
OLLAMA_BASE_URL=http://127.0.0.1:11434
OLLAMA_MODEL=exaone3.5:latest
PORT=8001

# instance2/.env
OLLAMA_BASE_URL=http://127.0.0.1:11435
OLLAMA_MODEL=llama3.2:latest
PORT=8002

# instance3/.env
OLLAMA_BASE_URL=http://127.0.0.1:11436
OLLAMA_MODEL=gemma2:9b
PORT=8003
```

## í¬íŠ¸ êµ¬ì„± ìš”ì•½

### ê¶Œì¥ í¬íŠ¸ í• ë‹¹

```
# Ollama í¬íŠ¸
11434 - Ollama ê¸°ë³¸ ì¸ìŠ¤í„´ìŠ¤
11435 - Ollama ì¸ìŠ¤í„´ìŠ¤ 2 (ì„ íƒì‚¬í•­)
11436 - Ollama ì¸ìŠ¤í„´ìŠ¤ 3 (ì„ íƒì‚¬í•­)

# LLM API ì„œë²„ í¬íŠ¸
8001  - LLM API Server 1 (exaone3.5)
8002  - LLM API Server 2 (llama3.2)
8003  - LLM API Server 3 (gemma2)
8004+ - ì¶”ê°€ ì¸ìŠ¤í„´ìŠ¤
```

### ë°©í™”ë²½ ì„¤ì •

```bash
# VM ì„œë²„ IPì—ì„œë§Œ ì ‘ê·¼ í—ˆìš©
sudo ufw allow from <VMì„œë²„_IP> to any port 8001
sudo ufw allow from <VMì„œë²„_IP> to any port 8002
sudo ufw allow from <VMì„œë²„_IP> to any port 8003

# ë˜ëŠ” iptables
sudo iptables -A INPUT -p tcp -s <VMì„œë²„_IP> --dport 8001:8003 -j ACCEPT
sudo iptables -A INPUT -p tcp --dport 8001:8003 -j DROP
```

## VM ì„œë²„ ì—°ê²°

### ë°©ë²• 1: í™˜ê²½ ë³€ìˆ˜ë¡œ íŠ¹ì • ì¸ìŠ¤í„´ìŠ¤ ì„ íƒ

VM ì„œë²„ì˜ `.env`:

```env
# ì¸ìŠ¤í„´ìŠ¤ 1 (exaone3.5) ì‚¬ìš©
GPU_LLM_URL=http://GPU_SERVER_IP:8001

# ë˜ëŠ” ì¸ìŠ¤í„´ìŠ¤ 2 (llama3.2) ì‚¬ìš©
# GPU_LLM_URL=http://GPU_SERVER_IP:8002

# ë˜ëŠ” ì¸ìŠ¤í„´ìŠ¤ 3 (gemma2) ì‚¬ìš©
# GPU_LLM_URL=http://GPU_SERVER_IP:8003
```

### ë°©ë²• 2: ë™ì ìœ¼ë¡œ ì¸ìŠ¤í„´ìŠ¤ ì„ íƒ (ê³ ê¸‰)

ì—¬ëŸ¬ VM ì„œë²„ì—ì„œ ê°ê¸° ë‹¤ë¥¸ ëª¨ë¸ ì‚¬ìš©:

**VM ì„œë²„ 1:**
```env
GPU_LLM_URL=http://GPU_SERVER_IP:8001  # exaone3.5
```

**VM ì„œë²„ 2:**
```env
GPU_LLM_URL=http://GPU_SERVER_IP:8002  # llama3.2
```

### ë°©ë²• 3: ë¡œë“œ ë°¸ëŸ°ì„œ ì‚¬ìš© (ìµœê³ ê¸‰)

ê°™ì€ ëª¨ë¸ì˜ ì—¬ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë¡œë“œ ë°¸ëŸ°ì‹±:

```bash
# Nginx ë¡œë“œ ë°¸ëŸ°ì„œ ì„¤ì •
upstream llm_backend {
    server 127.0.0.1:8001;
    server 127.0.0.1:8002;
    server 127.0.0.1:8003;
}

server {
    listen 8000;
    location / {
        proxy_pass http://llm_backend;
    }
}
```

VM ì„œë²„:
```env
GPU_LLM_URL=http://GPU_SERVER_IP:8000  # ë¡œë“œ ë°¸ëŸ°ì„œ
```

## í…ŒìŠ¤íŠ¸

### ê° ì¸ìŠ¤í„´ìŠ¤ í…ŒìŠ¤íŠ¸

```bash
# ì¸ìŠ¤í„´ìŠ¤ 1 (í¬íŠ¸ 8001)
curl http://GPU_SERVER_IP:8001/health
curl -X POST http://GPU_SERVER_IP:8001/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt":"ì•ˆë…•í•˜ì„¸ìš”","temperature":0.7}'

# ì¸ìŠ¤í„´ìŠ¤ 2 (í¬íŠ¸ 8002)
curl http://GPU_SERVER_IP:8002/health
curl -X POST http://GPU_SERVER_IP:8002/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt":"Hello","temperature":0.7}'

# ì¸ìŠ¤í„´ìŠ¤ 3 (í¬íŠ¸ 8003)
curl http://GPU_SERVER_IP:8003/health
curl -X POST http://GPU_SERVER_IP:8003/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt":"ä½ å¥½","temperature":0.7}'
```

## ëª¨ë‹ˆí„°ë§

### ëª¨ë“  ì¸ìŠ¤í„´ìŠ¤ ìƒíƒœ í™•ì¸

```bash
# Systemd ì„œë¹„ìŠ¤
sudo systemctl status qrchatbot-gpu-* --no-pager

# í”„ë¡œì„¸ìŠ¤
ps aux | grep "python run.py"

# í¬íŠ¸
sudo netstat -tulpn | grep -E ":(8001|8002|8003)"

# GPU ì‚¬ìš©ëŸ‰
nvidia-smi
watch -n 1 nvidia-smi
```

### ë¡œê·¸ í™•ì¸

```bash
# Systemd
sudo journalctl -u qrchatbot-gpu-8001 -f
sudo journalctl -u qrchatbot-gpu-8002 -f
sudo journalctl -u qrchatbot-gpu-8003 -f

# ì§ì ‘ ì‹¤í–‰
tail -f ~/qrchatbot-gpu/instance1/gpu-server-8001.log
tail -f ~/qrchatbot-gpu/instance2/gpu-server-8002.log
tail -f ~/qrchatbot-gpu/instance3/gpu-server-8003.log
```

## ë¦¬ì†ŒìŠ¤ ê´€ë¦¬

### GPU ë©”ëª¨ë¦¬ ê³ ë ¤ì‚¬í•­

**í•œ ë²ˆì— ë¡œë“œí•  ìˆ˜ ìˆëŠ” ëª¨ë¸ ìˆ˜ëŠ” GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ë‹¤ë¦…ë‹ˆë‹¤:**

- **24GB VRAM (RTX 3090/4090):**
  - 7B ëª¨ë¸ 2-3ê°œ ë™ì‹œ ê°€ëŠ¥
  - ë˜ëŠ” 13B ëª¨ë¸ 1ê°œ + 7B ëª¨ë¸ 1ê°œ

- **48GB VRAM (A6000):**
  - 7B ëª¨ë¸ 4-6ê°œ
  - ë˜ëŠ” 13B ëª¨ë¸ 2-3ê°œ

- **80GB VRAM (A100):**
  - 7B ëª¨ë¸ 8-10ê°œ
  - ë˜ëŠ” 70B ëª¨ë¸ 1ê°œ

### ëª¨ë¸ ì–¸ë¡œë“œ

OllamaëŠ” ê¸°ë³¸ì ìœ¼ë¡œ 5ë¶„ ë™ì•ˆ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë©´ ëª¨ë¸ì„ ì–¸ë¡œë“œí•©ë‹ˆë‹¤.

```bash
# ì¦‰ì‹œ ì–¸ë¡œë“œ
curl http://localhost:11434/api/generate -d '{
  "model": "exaone3.5:latest",
  "keep_alive": 0
}'
```

## ë¬¸ì œ í•´ê²°

### í¬íŠ¸ ì¶©ëŒ

```bash
# í¬íŠ¸ ì‚¬ìš© í™•ì¸
sudo lsof -i :8001
sudo lsof -i :8002
sudo lsof -i :8003

# í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ
sudo kill -9 <PID>
```

### GPU ë©”ëª¨ë¦¬ ë¶€ì¡±

```bash
# ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸ í™•ì¸
curl http://localhost:11434/api/ps

# íŠ¹ì • ëª¨ë¸ ì–¸ë¡œë“œ
curl http://localhost:11434/api/generate -d '{
  "model": "model-name",
  "keep_alive": 0
}'
```

### ì¸ìŠ¤í„´ìŠ¤ ì¬ì‹œì‘

```bash
# ì „ì²´ ì¬ì‹œì‘
sudo systemctl restart qrchatbot-gpu-8001 qrchatbot-gpu-8002 qrchatbot-gpu-8003

# ê°œë³„ ì¬ì‹œì‘
sudo systemctl restart qrchatbot-gpu-8001
```

## ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤

1. **ëª¨ë¸ ì„ íƒ**: ì‘ì€ ëª¨ë¸ë¶€í„° ì‹œì‘ (7B ëª¨ë¸)
2. **ëª¨ë‹ˆí„°ë§**: GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì§€ì† ëª¨ë‹ˆí„°ë§
3. **ë¡œë“œ ë°¸ëŸ°ì‹±**: ê°™ì€ ëª¨ë¸ì„ ì—¬ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ë¡œ ë¶€í•˜ ë¶„ì‚°
4. **ìë™ ì¬ì‹œì‘**: Systemdë¡œ ì„œë¹„ìŠ¤ ê´€ë¦¬
5. **ë¡œê·¸ ê´€ë¦¬**: ë¡œê·¸ ë¡œí…Œì´ì…˜ ì„¤ì •

---

**ì‘ì„±ì¼:** 2025-11-17  
**ë²„ì „:** 1.0.0

