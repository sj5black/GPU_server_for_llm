# GPU ì„œë²„ ë¶„ë¦¬ ê°€ì´ë“œ

QR ChatBot í”„ë¡œì íŠ¸ë¥¼ **VM ì„œë²„**(ì›¹/RAG)ì™€ **GPU ì„œë²„**(LLM ì¶”ë¡ )ë¡œ ë¶„ë¦¬í•˜ëŠ” ê°€ì´ë“œì…ë‹ˆë‹¤.

## ğŸ“‹ ëª©ì°¨

- [ê°œìš”](#ê°œìš”)
- [ì•„í‚¤í…ì²˜](#ì•„í‚¤í…ì²˜)
- [GPU ì„œë²„ ì„¤ì •](#gpu-ì„œë²„-ì„¤ì •)
- [VM ì„œë²„ ì„¤ì •](#vm-ì„œë²„-ì„¤ì •)
- [í…ŒìŠ¤íŠ¸](#í…ŒìŠ¤íŠ¸)
- [ë¬¸ì œ í•´ê²°](#ë¬¸ì œ-í•´ê²°)

## ê°œìš”

### ë¶„ë¦¬ ì „
```
ë‹¨ì¼ ì„œë²„
â”œâ”€â”€ FastAPI Backend (ì›¹, RAG, ë²¡í„° DB)
â”œâ”€â”€ Ollama LLM (GPU ì‚¬ìš©)
â”œâ”€â”€ Express Proxy
â””â”€â”€ Nginx
```

### ë¶„ë¦¬ í›„
```
VM ì„œë²„ (ì›¹/RAG)                GPU ì„œë²„ (LLM)
â”œâ”€â”€ FastAPI Backend          â† API í˜¸ì¶œ â†’ â”œâ”€â”€ LLM API Server
â”œâ”€â”€ RAG Service                              â”œâ”€â”€ Ollama
â”œâ”€â”€ ë²¡í„° DB (LanceDB)                        â””â”€â”€ GPU í™œìš©
â”œâ”€â”€ Express Proxy
â””â”€â”€ Nginx
```

### ì¥ì 

- âœ… GPU ë¦¬ì†ŒìŠ¤ íš¨ìœ¨ì  í™œìš©
- âœ… VM ì„œë²„ì™€ GPU ì„œë²„ ë…ë¦½ì  í™•ì¥
- âœ… LLM ì„œë²„ ì¥ì•  ì‹œ VM ì„œë²„ëŠ” ì •ìƒ ì‘ë™ (ì»¨í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜)
- âœ… ì—¬ëŸ¬ VM ì„œë²„ê°€ í•˜ë‚˜ì˜ GPU ì„œë²„ ê³µìœ  ê°€ëŠ¥
- âœ… LLM ëª¨ë¸ ë³€ê²½ ì‹œ VM ì„œë²„ ì¬ì‹œì‘ ë¶ˆí•„ìš”

## ì•„í‚¤í…ì²˜

### í†µì‹  íë¦„

```
ì‚¬ìš©ì ìš”ì²­
    â†“
Nginx (HTTPS)
    â†“
Express Proxy (í¬íŠ¸ 3000)
    â†“
FastAPI Backend (í¬íŠ¸ 8000)
    â†“
RAG Service (ë¬¸ì„œ ê²€ìƒ‰)
    â†“
LLM Client â† HTTP â†’ GPU Server (í¬íŠ¸ 11435)
                        â†“
                     Ollama (í¬íŠ¸ 11434)
                        â†“
                     LLM ì¶”ë¡ 
```

### API ì—”ë“œí¬ì¸íŠ¸

**GPU ì„œë²„ (í¬íŠ¸ 11435):**
- `GET /health` - í—¬ìŠ¤ ì²´í¬
- `GET /models` - ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡
- `POST /generate` - í…ìŠ¤íŠ¸ ìƒì„± (ì¼ë°˜)
- `POST /generate_stream` - í…ìŠ¤íŠ¸ ìƒì„± (ìŠ¤íŠ¸ë¦¬ë°)

**VM ì„œë²„:**
- ê¸°ì¡´ ëª¨ë“  ì—”ë“œí¬ì¸íŠ¸ ìœ ì§€
- RAG Serviceê°€ ë‚´ë¶€ì ìœ¼ë¡œ GPU ì„œë²„ í˜¸ì¶œ

## GPU ì„œë²„ ì„¤ì •

### 1. GPU ì„œë²„ì— í”„ë¡œì íŠ¸ ë³µì‚¬

GPU ì„œë²„ì—ëŠ” `gpu-server` í´ë”ë§Œ í•„ìš”í•©ë‹ˆë‹¤:

```bash
# GPU ì„œë²„ì—ì„œ
mkdir -p ~/qrchatbot-gpu
cd ~/qrchatbot-gpu

# gpu-server í´ë” ë³µì‚¬ (ë˜ëŠ” git clone í›„ í•´ë‹¹ í´ë”ë§Œ ì‚¬ìš©)
```

### 2. ì˜ì¡´ì„± ì„¤ì¹˜

```bash
cd ~/qrchatbot-gpu/gpu-server

# Python ê°€ìƒí™˜ê²½ ìƒì„± (ì„ íƒì‚¬í•­)
python3 -m venv venv
source venv/bin/activate

# íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install -r requirements.txt
```

### 3. Ollama ì„¤ì¹˜ ë° ëª¨ë¸ ë‹¤ìš´ë¡œë“œ

```bash
# Ollama ì„¤ì¹˜ (Linux)
curl -fsSL https://ollama.com/install.sh | sh

# Ollama ì„œë¹„ìŠ¤ ì‹œì‘
ollama serve &

# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
ollama pull exaone3.5:latest
# ë˜ëŠ” ë‹¤ë¥¸ ëª¨ë¸: ollama pull llama3.2:latest
```

### 4. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

```bash
# gpu-server ë””ë ‰í† ë¦¬ì— .env íŒŒì¼ ìƒì„±
cat > .env << EOF
# Ollama ì„¤ì •
OLLAMA_BASE_URL=http://127.0.0.1:11434
OLLAMA_MODEL=exaone3.5:latest

# ì„œë²„ ì„¤ì •
HOST=0.0.0.0
PORT=11435
EOF
```

### 5. GPU ì„œë²„ ì‹œì‘

```bash
# ì§ì ‘ ì‹¤í–‰
python run.py

# ë˜ëŠ” ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰
nohup python run.py > gpu-server.log 2>&1 &

# ë˜ëŠ” systemd ì„œë¹„ìŠ¤ë¡œ ì‹¤í–‰ (ê¶Œì¥)
```

#### Systemd ì„œë¹„ìŠ¤ íŒŒì¼ (ì„ íƒì‚¬í•­)

```bash
sudo nano /etc/systemd/system/qrchatbot-gpu.service
```

```ini
[Unit]
Description=QRChatBot GPU LLM Server
After=network.target

[Service]
Type=simple
User=your-username
WorkingDirectory=/home/your-username/qrchatbot-gpu/gpu-server
Environment="PATH=/home/your-username/qrchatbot-gpu/gpu-server/venv/bin"
ExecStart=/home/your-username/qrchatbot-gpu/gpu-server/venv/bin/python run.py
Restart=always
RestartSec=5
StandardOutput=journal
StandardError=journal

[Install]
WantedBy=multi-user.target
```

```bash
sudo systemctl daemon-reload
sudo systemctl enable qrchatbot-gpu
sudo systemctl start qrchatbot-gpu
sudo systemctl status qrchatbot-gpu
```

### 6. ë°©í™”ë²½ ì„¤ì •

GPU ì„œë²„ì˜ í¬íŠ¸ 11435ë¥¼ VM ì„œë²„ IPì—ì„œë§Œ ì ‘ê·¼ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ì •:

```bash
# UFW ì‚¬ìš© ì‹œ
sudo ufw allow from <VMì„œë²„_IP> to any port 11435

# iptables ì‚¬ìš© ì‹œ
sudo iptables -A INPUT -p tcp -s <VMì„œë²„_IP> --dport 11435 -j ACCEPT
```

### 7. í…ŒìŠ¤íŠ¸

```bash
# GPU ì„œë²„ì—ì„œ ë¡œì»¬ í…ŒìŠ¤íŠ¸
curl http://localhost:11435/health

# ì˜ˆìƒ ì¶œë ¥:
# {"status":"healthy","ollama_status":"connected",...}

# í…ìŠ¤íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸
curl -X POST http://localhost:11435/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt":"ì•ˆë…•í•˜ì„¸ìš”. ìê¸°ì†Œê°œë¥¼ í•´ì£¼ì„¸ìš”.","temperature":0.7}'
```

## VM ì„œë²„ ì„¤ì •

### 1. RAG Service ë³€ê²½

ê¸°ì¡´ `rag_service.py`ë¥¼ ìƒˆë¡œìš´ GPU ì„œë²„ìš© ë²„ì „ìœ¼ë¡œ êµì²´:

```bash
cd /home/mymeta_corp/QRchatbot/QRChatBot/app/services

# ê¸°ì¡´ íŒŒì¼ ë°±ì—…
cp rag_service.py rag_service.py.backup

# ìƒˆ ë²„ì „ìœ¼ë¡œ êµì²´
cp rag_service_gpu.py rag_service.py
```

ë˜ëŠ” ì‹¬ë³¼ë¦­ ë§í¬ ìƒì„±:

```bash
cd /home/mymeta_corp/QRchatbot/QRChatBot/app/services
rm rag_service.py
ln -s rag_service_gpu.py rag_service.py
```

### 2. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

VM ì„œë²„ì˜ `.env` íŒŒì¼ì— GPU ì„œë²„ URL ì¶”ê°€:

```bash
cd /home/mymeta_corp/QRchatbot/QRChatBot

# .env íŒŒì¼ í¸ì§‘
nano .env
```

ë‹¤ìŒ ë‚´ìš© ì¶”ê°€:

```env
# GPU LLM Server ì„¤ì •
GPU_LLM_URL=http://<GPUì„œë²„_IP>:11435

# ê¸°ì¡´ ì„¤ì •ì€ ê·¸ëŒ€ë¡œ ìœ ì§€
OLLAMA_BASE_URL=http://127.0.0.1:11434
OLLAMA_MODEL=exaone3.5:latest
```

**ì¤‘ìš”:** `GPU_LLM_URL`ì„ GPU ì„œë²„ì˜ ì‹¤ì œ IP ì£¼ì†Œë¡œ ë³€ê²½í•˜ì„¸ìš”.

### 3. VM ì„œë²„ ì¬ì‹œì‘

```bash
cd /home/mymeta_corp/QRchatbot/QRChatBot
sudo ./restart-all.sh
```

### 4. ì—°ê²° í™•ì¸

```bash
# VM ì„œë²„ì—ì„œ GPU ì„œë²„ ì—°ê²° í…ŒìŠ¤íŠ¸
curl http://<GPUì„œë²„_IP>:11435/health

# FastAPI ë¡œê·¸ í™•ì¸
tail -f logs/backend.log

# ë‹¤ìŒ ë©”ì‹œì§€ê°€ ë³´ì—¬ì•¼ í•¨:
# [OK] GPU LLM Client ì´ˆê¸°í™” ì™„ë£Œ: http://<GPUì„œë²„_IP>:11435
```

## í…ŒìŠ¤íŠ¸

### 1. GPU ì„œë²„ ë‹¨ë… í…ŒìŠ¤íŠ¸

```bash
# í—¬ìŠ¤ ì²´í¬
curl http://<GPUì„œë²„_IP>:11435/health

# ëª¨ë¸ ëª©ë¡
curl http://<GPUì„œë²„_IP>:11435/models

# í…ìŠ¤íŠ¸ ìƒì„± (ì¼ë°˜)
curl -X POST http://<GPUì„œë²„_IP>:11435/generate \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?",
    "temperature": 0.7
  }'

# í…ìŠ¤íŠ¸ ìƒì„± (ìŠ¤íŠ¸ë¦¬ë°)
curl -X POST http://<GPUì„œë²„_IP>:11435/generate_stream \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "í•œêµ­ì— ëŒ€í•´ ê°„ë‹¨íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
    "temperature": 0.7
  }'
```

### 2. ì „ì²´ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸

```bash
# VM ì„œë²„ë¥¼ í†µí•œ ì±—ë´‡ í…ŒìŠ¤íŠ¸
# 1. ë¸Œë¼ìš°ì €ì—ì„œ https://your-vm-server ì ‘ì†
# 2. ë¡œê·¸ì¸
# 3. í´ë” ìƒì„± ë° ë¬¸ì„œ ì—…ë¡œë“œ
# 4. ì±—ë´‡ì— ì§ˆë¬¸í•˜ê¸°
# 5. ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ í™•ì¸
```

### 3. ë¡œê·¸ í™•ì¸

**GPU ì„œë²„:**
```bash
# systemd ì‚¬ìš© ì‹œ
sudo journalctl -u qrchatbot-gpu -f

# ì§ì ‘ ì‹¤í–‰ ì‹œ
tail -f gpu-server.log
```

**VM ì„œë²„:**
```bash
tail -f logs/backend.log
```

## ë¬¸ì œ í•´ê²°

### GPU ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŒ

**ì¦ìƒ:**
```
[WARNING] LLM Client ì´ˆê¸°í™” ì‹¤íŒ¨
AI ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤
```

**í•´ê²°:**

1. **GPU ì„œë²„ ìƒíƒœ í™•ì¸:**
   ```bash
   # GPU ì„œë²„ì—ì„œ
   curl http://localhost:11435/health
   ```

2. **ë°©í™”ë²½ í™•ì¸:**
   ```bash
   # GPU ì„œë²„ì—ì„œ
   sudo ufw status
   # í¬íŠ¸ 11435ê°€ VM ì„œë²„ IPì—ì„œ í—ˆìš©ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
   ```

3. **ë„¤íŠ¸ì›Œí¬ ì—°ê²° í™•ì¸:**
   ```bash
   # VM ì„œë²„ì—ì„œ
   telnet <GPUì„œë²„_IP> 11435
   # ë˜ëŠ”
   nc -zv <GPUì„œë²„_IP> 11435
   ```

4. **GPU ì„œë²„ ë¡œê·¸ í™•ì¸:**
   ```bash
   sudo journalctl -u qrchatbot-gpu -n 100
   ```

### Ollama ì—°ê²° ì˜¤ë¥˜

**ì¦ìƒ:**
```
{"status":"unhealthy","ollama_status":"disconnected"}
```

**í•´ê²°:**

```bash
# GPU ì„œë²„ì—ì„œ Ollama ìƒíƒœ í™•ì¸
ollama list

# Ollama ì„œë¹„ìŠ¤ ì¬ì‹œì‘
pkill ollama
ollama serve &

# ëª¨ë¸ ë‹¤ì‹œ ë‹¤ìš´ë¡œë“œ
ollama pull exaone3.5:latest
```

### GPU ë©”ëª¨ë¦¬ ë¶€ì¡±

**ì¦ìƒ:**
```
CUDA out of memory
```

**í•´ê²°:**

1. **ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš©:**
   ```bash
   ollama pull llama3.2:latest  # ë” ì‘ì€ ëª¨ë¸
   ```

2. **ì–‘ìí™”ëœ ëª¨ë¸ ì‚¬ìš©:**
   ```bash
   ollama pull exaone3.5:8bit
   ```

3. **ë™ì‹œ ìš”ì²­ ìˆ˜ ì œí•œ:**
   GPU ì„œë²„ì˜ `main.py`ì—ì„œ ë™ì‹œ ìš”ì²­ ìˆ˜ ì œí•œ ì¶”ê°€

### ì‘ë‹µì´ ë„ˆë¬´ ëŠë¦¼

**ì¦ìƒ:**
LLM ì‘ë‹µì´ 10ì´ˆ ì´ìƒ ê±¸ë¦¼

**í•´ê²°:**

1. **GPU ì‚¬ìš© í™•ì¸:**
   ```bash
   nvidia-smi
   ```

2. **ëª¨ë¸ ìºì‹œ í™•ì¸:**
   ì²« ìš”ì²­ì€ ëª¨ë¸ ë¡œë”©ìœ¼ë¡œ ëŠë¦´ ìˆ˜ ìˆìŒ (ì •ìƒ)

3. **ë„¤íŠ¸ì›Œí¬ ì§€ì—° í™•ì¸:**
   ```bash
   ping <GPUì„œë²„_IP>
   ```

### VM ì„œë²„ê°€ GPU ì„œë²„ ì—†ì´ë„ ì‘ë™í•˜ê²Œ í•˜ê¸°

GPU ì„œë²„ ì¥ì•  ì‹œì—ë„ VM ì„œë²„ëŠ” ê³„ì† ì‘ë™í•©ë‹ˆë‹¤:
- ë¬¸ì„œ ê²€ìƒ‰ì€ ì •ìƒ ì‘ë™
- LLM ì‘ë‹µ ëŒ€ì‹  ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©ë§Œ ë°˜í™˜
- "AI ì„œë²„ ì—°ê²° ë¬¸ì œ" ë©”ì‹œì§€ í‘œì‹œ

## ì„±ëŠ¥ ìµœì í™”

### GPU ì„œë²„

1. **ë°°ì¹˜ ì²˜ë¦¬:**
   - ì—¬ëŸ¬ ìš”ì²­ì„ ë°°ì¹˜ë¡œ ì²˜ë¦¬ (í–¥í›„ êµ¬í˜„)

2. **ëª¨ë¸ ìºì‹±:**
   - Ollamaê°€ ìë™ìœ¼ë¡œ ëª¨ë¸ì„ ë©”ëª¨ë¦¬ì— ìºì‹±

3. **ì–‘ìí™”:**
   ```bash
   ollama pull exaone3.5:8bit  # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ
   ```

### VM ì„œë²„

1. **ì—°ê²° í’€:**
   - `llm_client.py`ê°€ ì´ë¯¸ httpx íƒ€ì„ì•„ì›ƒ ìµœì í™” ì ìš©

2. **ìºì‹±:**
   - RAG Serviceì˜ ë²¡í„°ìŠ¤í† ì–´ ìºì‹± (5ë¶„ TTL)

## ëª¨ë‹ˆí„°ë§

### GPU ì„œë²„ ëª¨ë‹ˆí„°ë§

```bash
# GPU ì‚¬ìš©ëŸ‰
watch -n 1 nvidia-smi

# ì„œë¹„ìŠ¤ ìƒíƒœ
sudo systemctl status qrchatbot-gpu

# ë¡œê·¸ ì‹¤ì‹œê°„ í™•ì¸
sudo journalctl -u qrchatbot-gpu -f

# í”„ë¡œì„¸ìŠ¤ í™•ì¸
ps aux | grep "python run.py"
```

### VM ì„œë²„ ëª¨ë‹ˆí„°ë§

```bash
# ì„œë¹„ìŠ¤ ìƒíƒœ
sudo ./restart-all.sh

# ë¡œê·¸ í™•ì¸
tail -f logs/backend.log

# GPU ì„œë²„ ì—°ê²° ìƒíƒœ
curl http://<GPUì„œë²„_IP>:11435/health
```

## ë¡¤ë°± (ì›ë˜ êµ¬ì¡°ë¡œ ë˜ëŒë¦¬ê¸°)

GPU ì„œë²„ ë¶„ë¦¬ ì „ ìƒíƒœë¡œ ëŒì•„ê°€ë ¤ë©´:

```bash
cd /home/mymeta_corp/QRchatbot/QRChatBot/app/services

# ë°±ì—…ì—ì„œ ë³µì›
cp rag_service.py.backup rag_service.py

# .envì—ì„œ GPU_LLM_URL ì œê±°
nano ../.env  # GPU_LLM_URL ë¼ì¸ ì‚­ì œ

# VM ì„œë²„ ì¬ì‹œì‘
cd /home/mymeta_corp/QRchatbot/QRChatBot
sudo ./restart-all.sh
```

## ì¶”ê°€ ì •ë³´

### í”„ë¡œë•ì…˜ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] GPU ì„œë²„ ë°©í™”ë²½ ì„¤ì • ì™„ë£Œ
- [ ] GPU ì„œë²„ systemd ì„œë¹„ìŠ¤ ë“±ë¡
- [ ] VM ì„œë²„ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
- [ ] GPU ì„œë²„ ëª¨ë‹ˆí„°ë§ ì„¤ì •
- [ ] ë°±ì—… ê³„íš ìˆ˜ë¦½
- [ ] ì¥ì•  ë³µêµ¬ ê³„íš ìˆ˜ë¦½
- [ ] ë¶€í•˜ í…ŒìŠ¤íŠ¸ ì™„ë£Œ

### ë³´ì•ˆ ê¶Œì¥ì‚¬í•­

1. **API í‚¤ ì¸ì¦ (ì„ íƒì‚¬í•­):**
   ```env
   # GPU ì„œë²„ .env
   API_KEY=your-secret-key
   ```

2. **VPN ë˜ëŠ” Private Network:**
   - GPU ì„œë²„ë¥¼ í¼ë¸”ë¦­ ì¸í„°ë„·ì— ë…¸ì¶œí•˜ì§€ ë§ˆì„¸ìš”
   - VPN ë˜ëŠ” í´ë¼ìš°ë“œ Private Network ì‚¬ìš© ê¶Œì¥

3. **TLS/SSL:**
   - í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œëŠ” GPU ì„œë²„ì—ë„ SSL ì ìš© ê³ ë ¤

## ë„ì›€ë§

ë¬¸ì œê°€ ì§€ì†ë˜ë©´ ë‹¤ìŒì„ í™•ì¸í•˜ì„¸ìš”:

1. GPU ì„œë²„ ë¡œê·¸
2. VM ì„œë²„ ë¡œê·¸
3. ë„¤íŠ¸ì›Œí¬ ì—°ê²°
4. ë°©í™”ë²½ ì„¤ì •
5. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

---

**ì‘ì„±ì¼:** 2025-11-17  
**ë²„ì „:** 1.0.0  
**ìƒíƒœ:** í”„ë¡œë•ì…˜ ì¤€ë¹„ ì™„ë£Œ
